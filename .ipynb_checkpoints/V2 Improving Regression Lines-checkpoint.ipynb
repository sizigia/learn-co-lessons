{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Regression Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we saw how after choosing the slope and y-intercept values of a regression line, we use the root mean squared error to distill the goodness of fit into one number.  \n",
    "\n",
    "Now we can go beyond that to find the \"best fit\" regression line by doing the following:\n",
    "* Adjust $b$ and $m$, as these are the only things that can vary in a single-variable regression line.\n",
    "* After each adjustment calculate the average squared error \n",
    "* The regression line (that is, the values of $b$ and $m$) with our smallest average squared error is our best fit line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this technique in action.  For this example, let's imagine that our data does not include the point when x = 0. This leaves our dataset looking like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_show = {'x': 100, 'y': 150}\n",
    "second_show = {'x': 200, 'y': 300}\n",
    "third_show = {'x': 400, 'y': 700}\n",
    "\n",
    "updated_shows = [first_show, second_show, third_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again take an initial guess at slope by drawing a line between the first and last points.  And then let's just take an initial stab at $b$ by setting $b$ = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slope_between_two_points(first_point, second_point):\n",
    "    return (second_point['y'] - first_point['y'])/(second_point['x'] - first_point['x'])\n",
    "\n",
    "slope_between_two_points(updated_shows[0], updated_shows[2]) # 1.833\n",
    "\n",
    "def regression_formula(x):\n",
    "    return 1.83*x + 50\n",
    "    # change the number 0 to different numbers, to see what happens\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we calculate the `root_mean_squared_error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27069.0"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def y(x, points):\n",
    "    point_at_x = list(filter(lambda point: point['x'] == x,points))[0]\n",
    "    return point_at_x['y']\n",
    "\n",
    "def squared_error(x, movies):\n",
    "    return (y(x, movies) - regression_formula(x))**2\n",
    "\n",
    "def sum_of_squared_errors(points):\n",
    "    squared_errors = list(map(lambda point: squared_error(point['x'], points), points))\n",
    "    return sum(squared_errors)\n",
    "\n",
    "sum_of_squared_errors(updated_shows) # 18956.33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, over 18,000.  Is that a good number? Who knows. Let's get a sense of this by plugging in different numbers for *b* and seeing what happens to the average squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| b        | residual sum of squared           | \n",
    "| ------------- |:-------------:| \n",
    "| 100      |53069| \n",
    "| 110      |55989 | \n",
    "| 90      |50749 | \n",
    "|80 | 49029\n",
    "|70 | 47909\n",
    "|60 | 47389\n",
    "| 50 | 47469"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice that simply by setting different numbers as $b$, we get a smaller residual sum of squares (RSS), given our value of $m$ at 1.83.  Setting $b$ to 110 produced a higher error, than at 100, so we tried moving in the other direction.  We kept moving our $b$ value lower until we set $b$ = 50, at which point our error increased from the value at 60.  So, we know that a value of $b$ between 50 and 60 produces the smallest RSS, when we set $m$ = 1.83. \n",
    "\n",
    "If we plot these two numbers on a chart, we get the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./cost-curve-plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see visually from this that the point when b = 60 is the lowest RSS. So we start at value 100, and we can move back and forth until we get to around 60.  The check of every ten is called our *step size*.  \n",
    "\n",
    "This technique is a called *gradient descent*.  Gradient just means *a series of successive changes* and that's what we do.  We successively change our value of our y-intercept.  We *descend*, as we descend along a cost curve.  When the value of our RSS no longer descends as we change our variable, we stop.\n",
    "\n",
    "So our technique from the top of this lesson holds true: \n",
    "\n",
    "* Adjust $b$ and $m$, as these are the only things that can vary in a single-variable regression line.\n",
    "* After each adjustment calculate the average squared error \n",
    "* The regression line (that is, the values of $b$ and $m$) with our smallest average squared error is our best fit line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still, things are not so simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things happened to work out fairly nicely for us, but it could have been worse. \n",
    "\n",
    "For example, imagine that instead of checking our error value by changing our y intercept by increasing and decreasing our y intercept by 10, we checked our y intercept every 80.  All of our plot points from 20 to 100 would disappear, and our graph would suggest something different.  Take a look, it skips right over our \"best fit\" value of 60, and suggests that if we move our b value higher than 100, our cost will continue to descend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./large-step.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find this exercise silly - who would check a difference in error every 100?  But remember with our linear regression, we do not just change our y-intercept but we also change our slope as we are changing the y intercept. So our calculations of residual sum of squares turn into a grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| b        |m = 1.83           | m = 1.9           | m = 2.0           | \n",
    "| ------------- |:-------------:| :-------------:| \n",
    "| 100      |53069| ? | ?\n",
    "| 110      |55989 |  ? | ? \n",
    "| 90      |50749 |  ? | ?\n",
    "|80 | 49029| ? | ?\n",
    "|70 | 47909 | ? | ?\n",
    "|60 | 47389 | ? | ?\n",
    "| 50 | 47469 | ? | ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much do we change our slope variable as we change our y-intercept?  If we choose too small of a number, we may never get there.  If we choose too large of a number, well we will hop right over our \"best fit\" value -- as the \"silly\" graph above showed.  So *this* is the problem that we need to solve: tell us how much to adjust $m$ and $b$ between each calculation of error, so that we can then arrive at our best fit line. \n",
    "\n",
    "To figure this out, we sent some all-intelligent mathematicians to go up to a mountain and when they came back down, they gave us the following formulas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b = b + $ average_error \n",
    "\n",
    "\n",
    "$m = $ m + average_error*sum_of_x_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are our gradient descent formulas.  Tada, as they say.\n",
    "\n",
    "Now, would we like to understand how to use those formulas, why those formulas make sense, and where they came from?  Yes, we would.  So let's do the following.  \n",
    "\n",
    "* First, let's explain these formulas a little more and see them in action\n",
    "* Then, hoping to be one of those smart mathematicians ourself, we'll develop some intuition for these formulas.  \n",
    "* Finally, we'll show how to use some good old fashioned mathematics to derive them.\n",
    "\n",
    "The material to come may be confusing at first.  But stick with it, we'll be attacking it from all angles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing our gradient descent formulas in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ok, so what are those formulas about?  Well, remember, that is the how much we should be changing our variables, as we calculate how the size of our error changes.\n",
    "\n",
    "What the first formula says is that we should change the b value by the amount of our average error.  b = b + average_error, means reassign b to equal the old value of b by the average_error.  \n",
    "\n",
    "The second formula says that as we do, we should also change our m value by the average_error of our function multiplied by x.  Making these two changes simultaneously can have a large effect on our line.  To combat this, we multiply these changes by a learning rate, for example, $0.1$. \n",
    "\n",
    "So really each step, looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "learning_rate = .1\n",
    "n = size_of_data_set\n",
    "b = b + learning_rate*(total_error)*(1/n)\n",
    "m = m + learning_rate*(total_error*sum_of_x_values)*(1/n)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So think of our formulas as telling us proportions, how much should we change $m$ as we change $b$.  These are factors of how large our error was, as well as our previous values of b and m, all multiplied by a learning rate so as not to overshoot the values that minimize our cost. \n",
    "\n",
    "Ok, let's turn this technique into code.  Our formulas above, depend on calculations of `average_error` and the `sum_of_x_values` for our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(points, x, m, b):\n",
    "    # actual y - expected y \n",
    "    return   - y(x, points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have those functions, we can move onto coding our formulas.  We said that those formulas were what happens in each step, so we wrap it in a function called step, and turn all of our variables into arguments, so that we can change these variables each time we execute our function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_show = {'x': 100, 'y': 150}\n",
    "second_show = {'x': 200, 'y': 300}\n",
    "third_show = {'x': 400, 'y': 700}\n",
    "\n",
    "updated_shows = [first_show, second_show, third_show]\n",
    "\n",
    "\n",
    "learning_rate = .001\n",
    "n = len(updated_shows)\n",
    "b = 100\n",
    "m = 1\n",
    "steps = []\n",
    "steps.append({'b': b, 'm': m})\n",
    "def step_gradient(b, m):\n",
    "    m_gradient = 0\n",
    "    b_gradient = 0\n",
    "    for show in updated_shows:\n",
    "        x = show['x'] \n",
    "        y = show['y']\n",
    "        expected = (m*x + b)\n",
    "        error = y - expected\n",
    "        b_gradient += (error)*(2/n)\n",
    "        m_gradient += (error*x)*(2/n)\n",
    "\n",
    "    b = b - learning_rate * b_gradient\n",
    "    m = m - learning_rate * m_gradient\n",
    "    return {'b': b, 'm': m, 'error': error, 'b_gradient': b_gradient, 'm_gradient': m_gradient}\n",
    "\n",
    "for i in range(10):\n",
    "    current_regression = steps[-1]\n",
    "    steps.append(step_gradient(current_regression['b'], current_regression['m']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'b': 100, 'm': 1},\n",
       " {'b': 99.9,\n",
       "  'b_gradient': 99.99999999999999,\n",
       "  'error': 200,\n",
       "  'm': -48.99999999999999,\n",
       "  'm_gradient': 49999.99999999999},\n",
       " {'b': 76.46646666666668,\n",
       "  'b_gradient': 23433.533333333326,\n",
       "  'error': 20200.099999999995,\n",
       "  'm': -7099.046666666665,\n",
       "  'm_gradient': 7050046.666666665},\n",
       " {'b': -3237.03571151111,\n",
       "  'b_gradient': 3313502.1781777767,\n",
       "  'error': 2840242.2001999994,\n",
       "  'm': -1001166.5623155553,\n",
       "  'm_gradient': 994067515.6488886},\n",
       " {'b': -470455.33886352653,\n",
       "  'b_gradient': 467218303.15201545,\n",
       "  'error': 400470561.9619336,\n",
       "  'm': -141166232.56982532,\n",
       "  'm_gradient': 140165066007.50977},\n",
       " {'b': -66348972.215459734,\n",
       "  'b_gradient': 65878516876.59621,\n",
       "  'error': 56466964183.269,\n",
       "  'm': -19904658574.83684,\n",
       "  'm_gradient': 19763492342267.016},\n",
       " {'b': -9355322339.183748,\n",
       "  'b_gradient': 9288973366968.29,\n",
       "  'error': 7961929779606.952,\n",
       "  'm': -2806587822142.362,\n",
       "  'm_gradient': 2786683163567525.0},\n",
       " {'b': -1319115016651.0645,\n",
       "  'b_gradient': 1309759694311880.5,\n",
       "  'error': 1122644484179983.9,\n",
       "  'm': -395733248739401.25,\n",
       "  'm_gradient': 3.929266609172589e+17},\n",
       " {'b': -185997269325072.38,\n",
       "  'b_gradient': 1.846781543084213e+17,\n",
       "  'error': 1.5829461861077786e+17,\n",
       "  'm': -5.579900365926358e+16,\n",
       "  'm_gradient': 5.540327041052418e+19},\n",
       " {'b': -2.622590430485339e+16,\n",
       "  'b_gradient': 2.603990703552832e+19,\n",
       "  'error': 2.2319787460974756e+19,\n",
       "  'm': -7.867746314681849e+18,\n",
       "  'm_gradient': 7.811947311022584e+21},\n",
       " {'b': -3.6978933029649925e+18,\n",
       "  'b_gradient': 3.6716673986601387e+21,\n",
       "  'error': 3.1471247517770444e+21,\n",
       "  'm': -1.1093644691254828e+21,\n",
       "  'm_gradient': 1.101496722810801e+24}]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with our `step` function, we alter $m$ and $b$ each time.  And then we recalculate `total_error()`.  Let's do this say, 30 times, and see where we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our initial step with m and b values, and the corresponding error \n",
    "\n",
    "\n",
    "\n",
    "def generate_gradient_descent(steps):\n",
    "    for i in range(10000):\n",
    "        \n",
    "        current_regression = steps[len(steps)-1]\n",
    "        \n",
    "        updated_regression = step(current_regression['b'], current_regression['m'], current_regression['total_error'])\n",
    "        steps.append(updated_regression)\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_steps = generate_gradient_descent(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'b': 998.5682740717915,\n",
       "  'm': -2.2081497460219643,\n",
       "  'total_error': 4.547473508864641e-13},\n",
       " {'b': 998.5682740717915,\n",
       "  'm': -2.2081497460219635,\n",
       "  'total_error': -1.1368683772161603e-13}]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulting_steps[998:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "x": [
          0,
          1
         ],
         "y": [
          0,
          1
         ]
        }
       ],
       "frames": [
        {
         "data": [
          {
           "x": [
            1,
            2
           ],
           "y": [
            1,
            2
           ]
          }
         ]
        },
        {
         "data": [
          {
           "x": [
            1,
            4
           ],
           "y": [
            1,
            4
           ]
          }
         ]
        },
        {
         "data": [
          {
           "x": [
            3,
            4
           ],
           "y": [
            3,
            4
           ]
          }
         ],
         "layout": {
          "title": "End Title"
         }
        }
       ],
       "layout": {
        "title": "Start Title",
        "updatemenus": [
         {
          "buttons": [
           {
            "args": [
             null
            ],
            "label": "Play",
            "method": "animate"
           }
          ],
          "type": "buttons"
         }
        ],
        "xaxis": {
         "autorange": false,
         "range": [
          0,
          5
         ]
        },
        "yaxis": {
         "autorange": false,
         "range": [
          0,
          5
         ]
        }
       }
      },
      "text/html": [
       "<div id=\"c400e391-d54c-45b0-8474-41f6b8def368\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            'c400e391-d54c-45b0-8474-41f6b8def368',\n",
       "            [{\"x\": [0, 1], \"y\": [0, 1]}],\n",
       "            {\"xaxis\": {\"range\": [0, 5], \"autorange\": false}, \"yaxis\": {\"range\": [0, 5], \"autorange\": false}, \"title\": \"Start Title\", \"updatemenus\": [{\"type\": \"buttons\", \"buttons\": [{\"label\": \"Play\", \"method\": \"animate\", \"args\": [null]}]}]},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('c400e391-d54c-45b0-8474-41f6b8def368',[{\"data\": [{\"x\": [1, 2], \"y\": [1, 2]}]}, {\"data\": [{\"x\": [1, 4], \"y\": [1, 4]}]}, {\"data\": [{\"x\": [3, 4], \"y\": [3, 4]}], \"layout\": {\"title\": \"End Title\"}}]);}).then(function(){Plotly.animate('c400e391-d54c-45b0-8474-41f6b8def368');})\n",
       "        });</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"c400e391-d54c-45b0-8474-41f6b8def368\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            'c400e391-d54c-45b0-8474-41f6b8def368',\n",
       "            [{\"x\": [0, 1], \"y\": [0, 1]}],\n",
       "            {\"xaxis\": {\"range\": [0, 5], \"autorange\": false}, \"yaxis\": {\"range\": [0, 5], \"autorange\": false}, \"title\": \"Start Title\", \"updatemenus\": [{\"type\": \"buttons\", \"buttons\": [{\"label\": \"Play\", \"method\": \"animate\", \"args\": [null]}]}]},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('c400e391-d54c-45b0-8474-41f6b8def368',[{\"data\": [{\"x\": [1, 2], \"y\": [1, 2]}]}, {\"data\": [{\"x\": [1, 4], \"y\": [1, 4]}]}, {\"data\": [{\"x\": [3, 4], \"y\": [3, 4]}], \"layout\": {\"title\": \"End Title\"}}]);}).then(function(){Plotly.animate('c400e391-d54c-45b0-8474-41f6b8def368');})\n",
       "        });</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "figure = {'data': [{'x': [0, 1], 'y': [0, 1]}],\n",
    "          'layout': {'xaxis': {'range': [0, 5], 'autorange': False},\n",
    "                     'yaxis': {'range': [0, 5], 'autorange': False},\n",
    "                     'title': 'Start Title',\n",
    "                     'updatemenus': [{'type': 'buttons',\n",
    "                                      'buttons': [{'label': 'Play',\n",
    "                                                   'method': 'animate',\n",
    "                                                   'args': [None]}]}]\n",
    "                    },\n",
    "          'frames': [{'data': [{'x': [1, 2], 'y': [1, 2]}]},\n",
    "                     {'data': [{'x': [1, 4], 'y': [1, 4]}]},\n",
    "                     {'data': [{'x': [3, 4], 'y': [3, 4]}],\n",
    "                      'layout': {'title': 'End Title'}}]}\n",
    "\n",
    "iplot(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the fitted line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_formula_variable(x, m, b):\n",
    "    return m*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we update our functions that calculate the error to use our new function, and to allow us to pass through the values of $b$ and $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 53789.0), (100, 53069.0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def squared_error_variable(point, m, b):\n",
    "    y_hat = regression_formula_variable(point['x'], m, b)\n",
    "    return (point['y'] - y_hat)**2\n",
    "\n",
    "def squared_errors_variable(points, m, b):\n",
    "    return list(map(lambda point: squared_error_variable(point, m, b), points))\n",
    "\n",
    "def sum_of_squared_error_variable(points, m, b):\n",
    "    return sum(squared_errors_variable(points, m, b))\n",
    "\n",
    "b_values = list(range(10, 180, 90)) \n",
    "errors = list(map(lambda b_value: sum_of_squared_error_variable(updated_shows, 1.83, b_value), b_values))\n",
    "error_chart = list(zip(b_values, errors))\n",
    "error_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is our error chart.  Note that it is identical to the data in the table we have above.  If we plot this data of the b-values, and the corresponding squared errors generated from them, we see that the data makes an curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "x": [
          10,
          100
         ],
         "y": [
          53789,
          53069
         ]
        }
       ],
       "layout": {
        "title": "Cost Function",
        "xaxis": {
         "title": "B value",
         "zeroline": false
        },
        "yaxis": {
         "title": "Sum Squared Error",
         "zeroline": false
        }
       }
      },
      "text/html": [
       "<div id=\"87f025da-44b7-4ccf-813b-2a83d45815be\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"87f025da-44b7-4ccf-813b-2a83d45815be\", [{\"type\": \"scatter\", \"x\": [10, 100], \"y\": [53789.0, 53069.0]}], {\"title\": \"Cost Function\", \"yaxis\": {\"zeroline\": false, \"title\": \"Sum Squared Error\"}, \"xaxis\": {\"zeroline\": false, \"title\": \"B value\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"87f025da-44b7-4ccf-813b-2a83d45815be\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"87f025da-44b7-4ccf-813b-2a83d45815be\", [{\"type\": \"scatter\", \"x\": [10, 100], \"y\": [53789.0, 53069.0]}], {\"title\": \"Cost Function\", \"yaxis\": {\"zeroline\": false, \"title\": \"Sum Squared Error\"}, \"xaxis\": {\"zeroline\": false, \"title\": \"B value\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly import graph_objs \n",
    "cost_function_trace = graph_objs.Scatter(\n",
    "    x=list(map(lambda error: error[0], error_chart)),\n",
    "    y=list(map(lambda error: error[1], error_chart)),\n",
    ")\n",
    "\n",
    "layout = dict(title = 'Cost Function',\n",
    "              yaxis = dict(zeroline = False, title= 'Sum Squared Error'),\n",
    "              xaxis = dict(zeroline = False, title= 'B value')\n",
    "             )\n",
    "plotly.offline.iplot(dict(data=[cost_function_trace], layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That smily face above, is called the **cost curve**.  It shows the errors of different levels of B.  We want to reduce the error, so to do that we need to find the value of b such that the sum of squared errors is lowest - that appears to be when b is 60.  So that means that our y intercept, when x is 1.83 should be 60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we show the regression line side by side of the points cost curve, you can see how the two numbers relate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Don't stress about the below code.  It's not important -- it's just used to generate lines in our plots.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_regression_line(ending_x, m, b):\n",
    "    y_hat = m*ending_x + b\n",
    "    return {\n",
    "    'type':'line',\n",
    "    'x0': 0,\n",
    "    'y0': b,\n",
    "    'x1': ending_x,\n",
    "    'y1': y_hat,\n",
    "    'xref': 'x1',\n",
    "    'yref': 'y1',\n",
    "    'line': {\n",
    "        'color': 'rgb(55, 128, 191)',\n",
    "        'width': 3,\n",
    "        }\n",
    "    }\n",
    "line = generate_regression_line(400, 1.8, 500)\n",
    "\n",
    "def generate_cost_line(errors, b):\n",
    "    return {\n",
    "    'type':'line',\n",
    "    'x0': b,\n",
    "    'y0': 0,\n",
    "    'x1': b,\n",
    "    'y1': max(errors),\n",
    "    'xref': 'x2',\n",
    "    'yref': 'y1',\n",
    "    'line': {\n",
    "        'color': 'rgb(55, 128, 191)',\n",
    "        'width': 3,\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now the below code, still doesn't need to be understood.  But do change the value of b, and see how the plots below adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "x": [
          10,
          40,
          70,
          100
         ],
         "xaxis": "x2",
         "y": [
          53789,
          48149,
          47909,
          53069
         ],
         "yaxis": "y2"
        },
        {
         "mode": "markers",
         "type": "scatter",
         "x": [
          100,
          200,
          400
         ],
         "xaxis": "x1",
         "y": [
          150,
          600,
          700
         ],
         "yaxis": "y1"
        }
       ],
       "layout": {
        "shapes": [
         {
          "line": {
           "color": "rgb(55, 128, 191)",
           "width": 3
          },
          "type": "line",
          "x0": 0,
          "x1": 400,
          "xref": "x1",
          "y0": 80,
          "y1": 800,
          "yref": "y1"
         },
         {
          "line": {
           "color": "rgb(55, 128, 191)",
           "width": 3
          },
          "type": "line",
          "x0": 80,
          "x1": 80,
          "xref": "x2",
          "y0": 0,
          "y1": 53789,
          "yref": "y1"
         }
        ],
        "xaxis1": {
         "anchor": "y1",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis1": {
         "anchor": "x1",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          1000
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ]
        }
       }
      },
      "text/html": [
       "<div id=\"c560f2ba-a6e1-4130-b123-97c48acd44a5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"c560f2ba-a6e1-4130-b123-97c48acd44a5\", [{\"type\": \"scatter\", \"x\": [10, 40, 70, 100], \"y\": [53789.0, 48149.0, 47909.0, 53069.0], \"xaxis\": \"x2\", \"yaxis\": \"y2\"}, {\"type\": \"scatter\", \"x\": [100, 200, 400], \"y\": [150, 600, 700], \"mode\": \"markers\", \"xaxis\": \"x1\", \"yaxis\": \"y1\"}], {\"xaxis1\": {\"domain\": [0.0, 0.45], \"anchor\": \"y1\"}, \"yaxis1\": {\"domain\": [0.0, 1.0], \"anchor\": \"x1\", \"range\": [0, 1000]}, \"xaxis2\": {\"domain\": [0.55, 1.0], \"anchor\": \"y2\"}, \"yaxis2\": {\"domain\": [0.0, 1.0], \"anchor\": \"x2\"}, \"shapes\": [{\"type\": \"line\", \"x0\": 0, \"y0\": 80, \"x1\": 400, \"y1\": 800.0, \"xref\": \"x1\", \"yref\": \"y1\", \"line\": {\"color\": \"rgb(55, 128, 191)\", \"width\": 3}}, {\"type\": \"line\", \"x0\": 80, \"y0\": 0, \"x1\": 80, \"y1\": 53789.0, \"xref\": \"x2\", \"yref\": \"y1\", \"line\": {\"color\": \"rgb(55, 128, 191)\", \"width\": 3}}]}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"c560f2ba-a6e1-4130-b123-97c48acd44a5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"c560f2ba-a6e1-4130-b123-97c48acd44a5\", [{\"type\": \"scatter\", \"x\": [10, 40, 70, 100], \"y\": [53789.0, 48149.0, 47909.0, 53069.0], \"xaxis\": \"x2\", \"yaxis\": \"y2\"}, {\"type\": \"scatter\", \"x\": [100, 200, 400], \"y\": [150, 600, 700], \"mode\": \"markers\", \"xaxis\": \"x1\", \"yaxis\": \"y1\"}], {\"xaxis1\": {\"domain\": [0.0, 0.45], \"anchor\": \"y1\"}, \"yaxis1\": {\"domain\": [0.0, 1.0], \"anchor\": \"x1\", \"range\": [0, 1000]}, \"xaxis2\": {\"domain\": [0.55, 1.0], \"anchor\": \"y2\"}, \"yaxis2\": {\"domain\": [0.0, 1.0], \"anchor\": \"x2\"}, \"shapes\": [{\"type\": \"line\", \"x0\": 0, \"y0\": 80, \"x1\": 400, \"y1\": 800.0, \"xref\": \"x1\", \"yref\": \"y1\", \"line\": {\"color\": \"rgb(55, 128, 191)\", \"width\": 3}}, {\"type\": \"line\", \"x0\": 80, \"y0\": 0, \"x1\": 80, \"y1\": 53789.0, \"xref\": \"x2\", \"yref\": \"y1\", \"line\": {\"color\": \"rgb(55, 128, 191)\", \"width\": 3}}]}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly\n",
    "from plotly import graph_objs, tools\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "fig = tools.make_subplots(rows=1, cols=2)\n",
    "\n",
    "\n",
    "\n",
    "cost_function_trace = graph_objs.Scatter(\n",
    "    x=list(map(lambda error: error[0], error_chart)),\n",
    "    y=list(map(lambda error: error[1], error_chart)),\n",
    ")\n",
    "fig.append_trace(cost_function_trace, 1, 2)\n",
    "\n",
    "scatter_trace = graph_objs.Scatter(\n",
    "    x=list(map(lambda show: show['x'], updated_shows)),\n",
    "    y=list(map(lambda show: show['y'], updated_shows)),\n",
    "    mode=\"markers\"\n",
    ")\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "### CHANGE THIS VALUE OF B\n",
    "\n",
    "b = 80\n",
    "##############\n",
    "\n",
    "cost_line = generate_cost_line(errors, b)\n",
    "regression_line = generate_regression_line(400, 1.8, b)\n",
    "\n",
    "fig.append_trace(scatter_trace, 1, 1)\n",
    "\n",
    "fig['layout'].update(shapes=[regression_line, cost_line])\n",
    "fig['layout']['yaxis1'].update(range=[0, 1000])\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As you change the value of *b*, the regression line moves up and down.  Also, as you change the value of *b* the vertical line along the cost curve shifts left or right, with the intersecting point being the value of b and corresponding sum of squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically adjusting a regression line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now so far we have improved our regression line, by manually adjusting our estimated y intercept, and seeing the least squares.  What if we wanted to use code to do this process automatically.\n",
    "\n",
    "Well what you can imagine us doing is adjusting our values of b.  Now we don't want to think about the squared errors anymore, because squared errors does not tell us if our estimates are too high or too low.  Instead look at what happens by considering absolute error.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 46.333333333333336),\n",
       " (20, 36.333333333333336),\n",
       " (30, 26.333333333333332),\n",
       " (40, 16.333333333333332),\n",
       " (50, 6.333333333333333),\n",
       " (60, -3.6666666666666665),\n",
       " (70, -13.666666666666666),\n",
       " (80, -23.666666666666668),\n",
       " (90, -33.666666666666664),\n",
       " (100, -43.666666666666664),\n",
       " (110, -53.666666666666664)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def regression_formula_variable(x, m, b):\n",
    "    return m*x + b\n",
    "\n",
    "def error_variable(point, m, b):\n",
    "    y_hat = regression_formula_variable(point['x'], m, b)\n",
    "    return (point['y'] - y_hat)\n",
    "\n",
    "def errors_variable(points, m, b):\n",
    "    return list(map(lambda point: error_variable(point, m, b), points))\n",
    "\n",
    "def average_error_variable(points, m, b):\n",
    "    return sum(errors_variable(points, m, b))/len(points)\n",
    "\n",
    "b_values = list(range(10, 120, 10)) # [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110]\n",
    "errors = list(map(lambda b_value: average_error_variable(updated_shows, 1.83, b_value), b_values))\n",
    "linear_error_chart = list(zip(b_values, errors))\n",
    "linear_error_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_subplots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-25bd2e9d7eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcost_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_cost_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_error_chart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mregression_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_regression_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0madd_cost_function_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_error_chart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0madd_scatter_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdated_shows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_subplots' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##############\n",
    "\n",
    "### CHANGE THIS VALUE OF M\n",
    "\n",
    "b = 40\n",
    "##############\n",
    "\n",
    "cost_line = generate_cost_line(linear_error_chart, b)\n",
    "regression_line = generate_regression_line(400, 1.8, b)\n",
    "fig = make_subplots()\n",
    "add_cost_function_trace(fig, linear_error_chart)\n",
    "add_scatter_plot(updated_shows)\n",
    "\n",
    "fig['layout'].update(shapes=[regression_line, cost_line])\n",
    "fig['layout']['yaxis1'].update(range=[0, 1000])\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the charts above, show something interesting.  Our regression line stays the same.  And our error function changes to be linear.  That is if we increase our value of *b*, we expect the average deviation to change by a similar amount.  So now imagine we take our first guess of b, and set b equal to 100.  You can see, from the plot of the average error on the right that this gives us an average error of -43.67.  So if changing our b value a certain amount, changes our average error by the same amount, we can simply decrease our b value by 43.67."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = average_error_variable(updated_shows, 1.83, b)\n",
    "\n",
    "b = b + error\n",
    "b # 56.33333333333333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is pretty cool, even by guessing our b value incorrectly the first time, we can simply look at the average error and make an adjustment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the slope value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have gotten a sense for how we can adjust our y-intercept value, let's see if we can take a similar with adjusting our slope.  We came up with an initial guess of our slope simply by drawing a line between two of our points, and using the slope for that line.  That gave us an slope of 1.83.  Now let's see if we can improve on that. \n",
    "\n",
    "Ok, so we adjusted our y-intercept value by seeing how a change in the y-intercept changed our mean error.  Let's see how a change in the slope changes our mean error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints = list(range(0, 30, 1))\n",
    "m_values = list(map(lambda x: x/10.0, ints))\n",
    "m_value_errors = list(map(lambda m_value: average_error_variable(updated_shows, m_value, 56.33), m_values))\n",
    "m_linear_error_chart = list(zip(m_values, m_value_errors))\n",
    "m_linear_error_chart[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3.0\n",
    "cost_line = generate_cost_line(errors, m)\n",
    "regression_line = generate_regression_line(400, m, 56.33)\n",
    "fig = make_subplots()\n",
    "add_cost_function_trace(fig, m_linear_error_chart)\n",
    "add_scatter_plot(updated_shows)\n",
    "\n",
    "fig['layout'].update(shapes=[regression_line, cost_line])\n",
    "fig['layout']['yaxis1'].update(range=[0, 1500])\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cost chart on the right is a little difficult to interpret, but the main thing to realize is altering our m value no longer alters our cost by something approaching an equal amount.  Now focus on the chart to the right.  What we want to consider is how points further to the right (that is, with a larger x-coordinate) influence our error, as we change the slope of the line.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when the slope is 2.0, the point with x-value at 100 and x-value at 400, both miss the mark by say 100 or so.  Ok, go ahead and change the slope from 2.0 to say 3.0.  The error at x=100 rises by about 50 or so, but the error at point 400 rises by what, another 300?  A lot.  The takeaway point is that the change in the error as we change the slope of the line, does not influence all of the points equally.  The higher the x-value of a point, the more sensitive the error is to a changing slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we update m, we `m = m + error * x`, as we adjust our value of m not just by the average error, but also by our points' x-coordinate, as the further the x-coordinate the larger the error for a given point.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
